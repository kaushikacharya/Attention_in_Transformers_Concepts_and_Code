{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Masked Self-Attention in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "In this tutorial, we will code **Masked Self-Attention** in **[PyTorch](https://pytorch.org/)**. **Attention** is an essential component of neural network **Transformers**, which are driving the current excitement in **Large Language Models** and AI. Specifically, a **Decoder-Only Transformer**, illustrated below, is the foundation for the popular model **ChatGPT**. \n",
    "\n",
    "<img src=\"../images/dec_transformer.png\" alt=\"a decoder-only transformer neural network\" style=\"width: 800px;\">\n",
    "\n",
    "At the heart of **ChatGPT** is **Masked Self-Attention**, which allows it to establish relationships among the words, characters and symbols while also making it relatively easy to train the model to predict what should come next. For example, in the illustration below, where the word **it** could potentially refer to either **pizza** or **oven**, **Masked Self-Attention** could help a **Transformer** establish the correctly relationship between the word **it** and **pizza** as well as help efficiently train it to predict the words that came after, **tasted good**.\n",
    "\n",
    "<img src=\"../images/masked_self_attention_1.png\" alt=\"an illustration of how masked self-attention works\" style=\"width: 800px;\">\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Code a Masked Self-Attention Class!!!](#masked)** The the masked self-attention class allows the transformer to establish relationships among words and tokens and be efficiently trained to predict what comes next.\n",
    "\n",
    "- **[Calculate Masked Self-Attention Values!!!](#calculate)** We'll then use the class that we created, MaskedSelfAttention, to calculate masked self-attention values for some sample data.\n",
    " \n",
    "- **[Verify The Calculations!!!](#validate)** Lastly, we'll validate the calculations made by the MaskedSelfAttention class..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules that will do all the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F ## This gives us the softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Masked Self-Attention\n",
    "<a id=\"masked\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
    "        ## d_model = the number of embedding values per token.\n",
    "        super().__init__()\n",
    "\n",
    "        ## Initialize the Weights (W) that we'll use to create the\n",
    "        ## query (q), key (k) and value (v) for each token\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "    \n",
    "    def forward(self, token_embeddings, mask=None):\n",
    "        ## Create the query, key and values using the encoding numbers\n",
    "        ## associated with each token (token encodings)\n",
    "        q = self.W_q(token_embeddings)\n",
    "        k = self.W_k(token_embeddings)\n",
    "        v = self.W_v(token_embeddings)\n",
    "\n",
    "        ## Compute similarity scores: (q * k^T)\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        ## Scale the similarities by dividing by sqrt(k.col_dim)\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            ## Here we are masking out things we don't want to pay attention to\n",
    "            ##\n",
    "            ## We replace values we wanted masked out\n",
    "            ## with a very small negative number so that the SoftMax() function\n",
    "            ## will give all masked elements an output value (or \"probability\") of 0.\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9) # I've also seen -1e20 and -9e15 used in masking\n",
    "\n",
    "        ## Apply softmax to determine what percent of each tokens' value to\n",
    "        ## use in the final attention values.\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        ## Scale the values by their associated percentages and add them up.\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Masked Self-Attention\n",
    "<a id=\"calculate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a matrix of token encodings\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create a masked self-attention object\n",
    "maskedSelfAttention = MaskedSelfAttention(d_model=2,\n",
    "                                          row_dim=0,\n",
    "                                          col_dim=1)\n",
    "\n",
    "## create the mask so that we don't use\n",
    "## tokens that come after a token of interest\n",
    "mask = torch.tril(torch.ones(encodings_matrix.size(0), encodings_matrix.size(0)))\n",
    "mask = mask == 0\n",
    "\n",
    "## print the mask\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.0062,  0.6072],\n",
       "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskedSelfAttention(encodings_matrix, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Out Weights and Verify Calculations\n",
    "<a id=\"validate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5406, -0.1657],\n",
       "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print out the weight matrix that creates the queries\n",
    "maskedSelfAttention.W_q.weight.transpose(dim0=0, dim1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1549, -0.3443],\n",
       "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print out the weight matrix that creates the keys\n",
    "maskedSelfAttention.W_k.weight.transpose(dim0=0, dim1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6233,  0.6146],\n",
       "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print out the weight matrix that creates the values\n",
    "maskedSelfAttention.W_v.weight.transpose(dim0=0, dim1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7621, -0.0428],\n",
       "        [ 1.1063,  0.7890],\n",
       "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate the queries\n",
    "q = maskedSelfAttention.W_q(encodings_matrix)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1469, -0.3038],\n",
       "        [ 0.1057,  0.3685],\n",
       "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate the keys\n",
    "k = maskedSelfAttention.W_k(encodings_matrix)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.3502,  0.5303],\n",
       "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate the values\n",
    "v = maskedSelfAttention.W_v(encodings_matrix)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0990,  0.0648, -0.6523],\n",
       "        [-0.4022,  0.4078, -3.0024],\n",
       "        [ 0.4842, -0.6683,  4.0461]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = torch.matmul(q, k.transpose(dim0=0, dim1=1))\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0700,  0.0458, -0.4612],\n",
       "        [-0.2844,  0.2883, -2.1230],\n",
       "        [ 0.3424, -0.4725,  2.8610]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_sims = sims / (torch.tensor(k.size(1))**0.5)\n",
    "scaled_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.9975e-02, -1.0000e+09, -1.0000e+09],\n",
       "        [-2.8442e-01,  2.8833e-01, -1.0000e+09],\n",
       "        [ 3.4241e-01, -4.7253e-01,  2.8610e+00]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "masked_scaled_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.3606, 0.6394, 0.0000],\n",
       "        [0.0722, 0.0320, 0.8959]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_percents = F.softmax(masked_scaled_sims, dim=1)\n",
    "attention_percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.0062,  0.6072],\n",
       "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = torch.matmul(attention_percents, v)\n",
    "attention_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_attention_in_transformers_concepts_and_code_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
