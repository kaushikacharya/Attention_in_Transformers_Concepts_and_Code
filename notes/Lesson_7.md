# Encoder-Decoder Attention

- The first transformer model had both the Encoder and Decoder components.
- These were connected to each other so they could calculate **Encoder-Decoder Attention**.
- **Keys** and **Values** are calculated from the output of the **Encoder**.
- The **Queries** are calculated from the output of the **Masked Self-Attention** generated by the **Decoder**.
- Once the **Queries**, **Keys** and **Values** are calculated, **Encoder-Decoder Attention** is calculated just like **Self-Attention**, using every similarity.
- **Encoder-Decoder Attention** is also called **Cross-Attention**.

## Multi-Modal Models

- This style of **Seq2Seq** model has somewhat fallen out of favor for language modeling.
- But still seen in **Multi-Modal Models**
- Example:
  - **Encoder** trained on images or sound
  - The **Context Aware Embeddings** could be fed into a text based **Decoder** via **Encoder-Decoder Attention** in order to generate captions or respond to audio prompts.
